{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc76440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. 加载模型和分词器\n",
    "# model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    output_attentions=True\n",
    ").half().eval()\n",
    "\n",
    "# 2. 加载数据集\n",
    "triviaqa_dataset = load_dataset('THUDM/LongBench', 'triviaqa', split='test')\n",
    "num_samples = 10  # 取100条数据\n",
    "dataset_samples = triviaqa_dataset.select(range(num_samples))\n",
    "\n",
    "# 3. 定义参数\n",
    "num_layers = len(model.model.layers)\n",
    "num_heads = 32  # Qwen2-7B-Instruct的注意力头数\n",
    "kv_pairs = 32    # Multi-Query Attention的kv对数\n",
    "group_num = 10  # 分组数量\n",
    "\n",
    "# 初始化累计结果\n",
    "total_group_results = torch.zeros(num_layers, kv_pairs, group_num, device=model.device)\n",
    "\n",
    "# dataset = load_dataset('THUDM/LongBench', 'samsum', split='test')\n",
    "# random_indices = random.sample(range(len(dataset)), 10)\n",
    "# contexts = [dataset[idx]['context'] for idx in random_indices]\n",
    "# questions = [dataset[idx]['input'] for idx in random_indices]\n",
    "\n",
    "# # Create prompt for the first sequence\n",
    "# prompt_template = lambda c, q: f\"Context: {c}\\n\\nQuestion: {q}\\n\\nAnalyze the context and answer the question.\"\n",
    "\n",
    "# 4. 定义处理单个样本的函数\n",
    "def process_sample(sample):\n",
    "    question = sample[\"input\"]\n",
    "    evidence = sample[\"context\"]\n",
    "\n",
    "    prompt = f\"\"\"Answer the question based on the given passage. Only give me the answer and do not output any other words.\n",
    "\n",
    "Context: {evidence}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    seq_len = len(tokens)-5\n",
    "    \n",
    "    # 存储各层key向量和注意力权重\n",
    "    key_vectors = {}\n",
    "    attention_weights = {}\n",
    "\n",
    "    def get_key_hook(layer_idx):\n",
    "        def hook(module, input, output):\n",
    "            key_vectors[f\"layer_{layer_idx}\"] = output[2].detach()[:,:,5:,:] # key向量\n",
    "            attention_weights[f\"layer_{layer_idx}\"] = output[1].detach()[:,:,5:,5:]  # 注意力权重\n",
    "        return hook\n",
    "\n",
    "    # 注册钩子到每个注意力层\n",
    "    hooks = []\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        hook = layer.self_attn.register_forward_hook(get_key_hook(layer_idx))\n",
    "        hooks.append(hook)\n",
    "\n",
    "    # 执行前向传播\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # 移除钩子\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # 初始化当前样本的结果\n",
    "    sample_group_results = torch.zeros(num_layers, kv_pairs, group_num, device=model.device)\n",
    "\n",
    "    for layer_idx in range(num_layers):\n",
    "        layer_key = key_vectors[f\"layer_{layer_idx}\"]  # [1, kv_pairs, seq_len, head_dim]\n",
    "        layer_attn = attention_weights[f\"layer_{layer_idx}\"]  # [1, num_heads, seq_len, seq_len]\n",
    "        \n",
    "        # 计算key相似度\n",
    "        neighbor_mask = torch.zeros(seq_len, seq_len, dtype=torch.bool, device=model.device)\n",
    "        for i in range(seq_len):\n",
    "            left = max(0, i - 2)\n",
    "            right = min(seq_len, i + 3)\n",
    "            neighbor_mask[i, left:right] = True\n",
    "        \n",
    "        key_norm = layer_key.squeeze(0).norm(dim=2, keepdim=True)  # [kv_pairs, seq_len, 1]\n",
    "        cos_sim = torch.bmm(layer_key.squeeze(0), layer_key.squeeze(0).transpose(1,2)) / (key_norm * key_norm.transpose(1,2) + 1e-9)\n",
    "        \n",
    "        neighbor_sim = (cos_sim * neighbor_mask.unsqueeze(0)).sum(dim=2) / neighbor_mask.sum(dim=1).unsqueeze(0)\n",
    "        avg_sim = neighbor_sim.mean(dim=0)  # [seq_len]\n",
    "        \n",
    "        # 分组处理\n",
    "        sorted_indices = torch.argsort(avg_sim)\n",
    "        group_size = seq_len // group_num\n",
    "        groups = [sorted_indices[i*group_size : (i+1)*group_size] for i in range(group_num)]\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        last_10_tokens = slice(seq_len-10, seq_len)  # 最后10个token\n",
    "        prev_tokens = slice(None, seq_len-10)        # 前面所有token\n",
    "        \n",
    "        head_per_kv = num_heads // kv_pairs\n",
    "        relevant_heads = layer_attn.squeeze(0).reshape(kv_pairs, head_per_kv, seq_len, seq_len)\n",
    "        \n",
    "        avg_attn = relevant_heads[:, :, last_10_tokens, prev_tokens].mean(dim=1)\n",
    "        \n",
    "        for group_idx, group in enumerate(groups):\n",
    "            valid_group = group[group < seq_len-10]\n",
    "            if len(valid_group) > 0:\n",
    "                group_avg = avg_attn[:, :, valid_group].sum(dim=2)\n",
    "                sample_group_results[layer_idx, :, group_idx] = group_avg.mean(dim=1)\n",
    "    \n",
    "    return sample_group_results\n",
    "\n",
    "# 5. 处理所有样本\n",
    "for sample in tqdm(dataset_samples, desc=\"Processing samples\"):\n",
    "    try:\n",
    "        sample_result = process_sample(sample)\n",
    "        total_group_results += sample_result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample: {e}\")\n",
    "        continue\n",
    "\n",
    "# 6. 计算平均值\n",
    "avg_group_results = total_group_results / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be59eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize, LinearSegmentedColormap\n",
    "\n",
    "# Set Nature-inspired style parameters\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'Arial',\n",
    "    'font.size': 10,\n",
    "    'axes.labelsize': 14+2,\n",
    "    'axes.titlesize': 14+2,\n",
    "    'xtick.labelsize': 12+2,\n",
    "    'ytick.labelsize': 12+2,\n",
    "    'legend.fontsize': 12+2,\n",
    "    'figure.dpi': 600,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False\n",
    "})\n",
    "\n",
    "# Custom blue colormap\n",
    "colors = [\"#caf0f8\", \"#48cae4\", \"#0077b6\", \"#023e8a\", \"#001233\"]   # 更深的深蓝色和更浅的浅蓝色\n",
    "cmap = LinearSegmentedColormap.from_list(\"custom_blue\", colors, N=32)\n",
    "norm = Normalize(vmin=0, vmax=31)\n",
    "\n",
    "# Load data and select specific layers (0, 6, 12, 18, 24, 30)\n",
    "data = avg_group_results.cpu()\n",
    "selected_data = data[[0, 1, 4, 12, 20, 28]]  # 0-indexed layers 0,6,12,18,24,30\n",
    "\n",
    "# Create x-axis (Neighborhood Similarity 0.1-1)\n",
    "x = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# Layer titles\n",
    "layer_titles = ['Layer 1', 'Layer 2', 'Layer 4', 'Layer 12', 'Layer 20', 'Layer 28']\n",
    "\n",
    "# Create 2x3 subplots with adjusted size\n",
    "fig, axs = plt.subplots(2, 3, figsize=(7.5, 3.5))  # Wider figure for 3 columns\n",
    "plt.subplots_adjust(wspace=0.23, hspace=0.45)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axs_flat = axs.flatten()\n",
    "\n",
    "# Plot each subplot\n",
    "for i, ax in enumerate(axs_flat):\n",
    "    # Plot each attention head\n",
    "    for j in range(32):\n",
    "        ax.plot(x, selected_data[i,j], \n",
    "               marker='o', markersize=6, linestyle='-', linewidth=2.2,\n",
    "               color=cmap(norm(j)),\n",
    "               markerfacecolor='white',\n",
    "               markeredgewidth=0.8,\n",
    "               alpha=0.8)\n",
    "    \n",
    "    # Set titles and labels\n",
    "    ax.set_title(layer_titles[i], fontsize=14, fontweight=\"bold\")\n",
    "    \n",
    "    # Only left subplots get y-label\n",
    "    if i % 3 == 0:  # Changed from 2 to 3 for 3 columns\n",
    "        ax.set_ylabel('Attention', fontsize=12+2)\n",
    "    \n",
    "    # Bottom row gets x-label\n",
    "    if i >= 3:  # Changed from 2 to 3 for 2 rows\n",
    "        ax.set_xlabel('Localness Degree', fontsize=12+2)\n",
    "    \n",
    "    # Set ticks and grid\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "    ax.grid(True, linestyle=':', alpha=0.3)\n",
    "    ax.set_xticks([2, 4, 6, 8, 10])\n",
    "\n",
    "# Add horizontal colorbar at top\n",
    "cbar_ax = fig.add_axes([0.15, -0.12, 0.7, 0.02])  # [left, bottom, width, height]\n",
    "sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, cax=cbar_ax, orientation='horizontal')\n",
    "\n",
    "# Add colorbar label\n",
    "cbar.ax.text(15, 2.1, 'Attention Head Index From 1 To 32', \n",
    "             fontsize=12+3, ha='center', va='center')\n",
    "cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "# Save and show\n",
    "plt.savefig('layers_attention.pdf', bbox_inches='tight', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d666ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
